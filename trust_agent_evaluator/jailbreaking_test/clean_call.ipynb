{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import langgraph, langchain_openai, langchain_core\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class clean_call_test:\n",
    "    def __init__(self, llm, graph_file):\n",
    "\n",
    "        self.llm = llm    \n",
    "\n",
    "        with open(graph_file, 'r', encoding='utf-8') as f:\n",
    "            self.graph_data = json.load(f)\n",
    "            self.node = self.graph_data['nodes']\n",
    "\n",
    "    def test_clean_call(self):\n",
    "        # Initialize or load existing results\n",
    "        try:\n",
    "            with open('clean_call_results.json', 'r', encoding='utf-8') as f:\n",
    "                all_results = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            all_results = {\"clean_call_results\": []}\n",
    "\n",
    "        for node in tqdm(self.node, desc=\"Testing nodes clean call\"):\n",
    "            node_clean_call_results = []\n",
    "            \n",
    "            output = self.llm.invoke(node['input'])\n",
    "        \n",
    "            # Convert AIMessage to string if needed\n",
    "            if hasattr(output, 'content'):\n",
    "                output_str = output.content\n",
    "            else:\n",
    "                output_str = str(output)\n",
    "\n",
    "            result = {\n",
    "                \"output\": output_str\n",
    "            }\n",
    "\n",
    "            node_clean_call_results.append(result)\n",
    "\n",
    "                # Save results after each jailbreak attempt\n",
    "            node_result = {\n",
    "                \"id\": node['id'],\n",
    "                \"clean_call\": node_clean_call_results\n",
    "            }\n",
    "                \n",
    "            all_results[\"clean_call_results\"].append(node_result)\n",
    "                \n",
    "            # Write to file\n",
    "            with open('clean_call_results.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_results, f, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = langchain_openai.ChatOpenAI(\n",
    "                    model=\"deepseek/deepseek-chat\",\n",
    "                    openai_api_key=os.getenv(\"DEEPSEEK_OPENROUTER_API_KEY\"),\n",
    "                    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "                    temperature=0.2\n",
    "                )\n",
    "\n",
    "clean_call_test = clean_call_test(llm, '../data/agentic_graph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/15 09:17:01 INFO mlflow.tracking.fluent: Experiment with name 'clean_call' does not exist. Creating a new experiment.\n",
      "Testing nodes clean call:   0%|          | 0/16 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:   6%|▋         | 1/16 [00:25<06:22, 25.48s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  12%|█▎        | 2/16 [00:55<06:31, 27.95s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  19%|█▉        | 3/16 [01:46<08:23, 38.76s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  25%|██▌       | 4/16 [02:09<06:31, 32.59s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  31%|███▏      | 5/16 [03:01<07:12, 39.28s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  38%|███▊      | 6/16 [03:36<06:19, 37.91s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  44%|████▍     | 7/16 [04:27<06:19, 42.19s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  50%|█████     | 8/16 [05:14<05:49, 43.74s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  56%|█████▋    | 9/16 [05:48<04:45, 40.85s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  62%|██████▎   | 10/16 [06:29<04:04, 40.72s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  69%|██████▉   | 11/16 [07:01<03:11, 38.27s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  75%|███████▌  | 12/16 [07:25<02:15, 33.81s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  81%|████████▏ | 13/16 [07:34<01:19, 26.39s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  88%|████████▊ | 14/16 [07:49<00:45, 22.97s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call:  94%|█████████▍| 15/16 [08:23<00:26, 26.18s/it]INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Testing nodes clean call: 100%|██████████| 16/16 [08:32<00:00, 32.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=d16cc362d7884a7784d399ae17ad0f1c&amp;experiment_id=451998203895755627&amp;trace_id=4429f6ebb84c4a54acddd87026e1ee7c&amp;experiment_id=451998203895755627&amp;trace_id=1848b7adeef048faa0d82a8fbb2d3ac1&amp;experiment_id=451998203895755627&amp;trace_id=c3b9fb91272b4e729735f2d2b890dfa5&amp;experiment_id=451998203895755627&amp;trace_id=769aef905b9d49c8a8458aa18934bfdf&amp;experiment_id=451998203895755627&amp;trace_id=feab8010dc4849a480410debfe5847bc&amp;experiment_id=451998203895755627&amp;trace_id=10aebc03ffa744d19f3747571d24b909&amp;experiment_id=451998203895755627&amp;trace_id=6ba9b2f0a6df4b9db13df9925938ac31&amp;experiment_id=451998203895755627&amp;trace_id=26a5588cbaed4fd8b83f321f82c1dda0&amp;experiment_id=451998203895755627&amp;trace_id=af37e46bb9954b4186bbb53e754abafd&amp;experiment_id=451998203895755627&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=d16cc362d7884a7784d399ae17ad0f1c), Trace(request_id=4429f6ebb84c4a54acddd87026e1ee7c), Trace(request_id=1848b7adeef048faa0d82a8fbb2d3ac1), Trace(request_id=c3b9fb91272b4e729735f2d2b890dfa5), Trace(request_id=769aef905b9d49c8a8458aa18934bfdf), Trace(request_id=feab8010dc4849a480410debfe5847bc), Trace(request_id=10aebc03ffa744d19f3747571d24b909), Trace(request_id=6ba9b2f0a6df4b9db13df9925938ac31), Trace(request_id=26a5588cbaed4fd8b83f321f82c1dda0), Trace(request_id=af37e46bb9954b4186bbb53e754abafd)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.langchain.autolog()\n",
    "mlflow.set_experiment(\"clean_call\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "clean_call_test.test_clean_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
