{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# Define a helper for each of the agent nodes to call\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_travel_advisor():\n",
    "    \"\"\"Ask travel advisor for help.\"\"\"\n",
    "    # This tool is not returning anything: we're just using it\n",
    "    # as a way for LLM to signal that it needs to hand off to another agent\n",
    "    # (See the paragraph above)\n",
    "    return\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_hotel_advisor():\n",
    "    \"\"\"Ask hotel advisor for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def travel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"hotel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n",
    "        \"If you need hotel recommendations, ask 'hotel_advisor' for help.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_hotel_advisor]).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"hotel_advisor\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "def hotel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n",
    "        \"If you need help picking travel destinations, ask 'travel_advisor' for help.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_travel_advisor]).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"travel_advisor\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"travel_advisor\", travel_advisor)\n",
    "builder.add_node(\"hotel_advisor\", hotel_advisor)\n",
    "# we'll always start with a general travel advisor\n",
    "builder.add_edge(START, \"travel_advisor\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"i wanna go somewhere warm in the caribbean. pick one destination and give me hotel recommendations\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True,\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_in_one_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
