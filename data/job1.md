# AI Research Scientist (Alignment)

## About Anthropic  
Anthropic’s mission is to create reliable, interpretable, and steerable AI systems that are safe and beneficial for users and society. As part of our Alignment Science team, you’ll contribute to exploratory experimental research on AI safety, with a focus on risks from powerful future systems. We work collaboratively across teams, including Interpretability, Fine-Tuning, and the Frontier Red Team, to advance our understanding of AI alignment.

---

## Key Responsibilities  

### Research Areas  
- **AI Control**: Develop methods to ensure advanced AI systems remain safe and harmless in unfamiliar or adversarial scenarios.  
- **Alignment Stress-Testing**: Build model organisms of misalignment to understand and prevent alignment failures.  

### Representative Projects  
- Test the robustness of safety techniques by training language models to subvert them and evaluating interventions.  
- Conduct multi-agent reinforcement learning experiments to explore techniques like AI Debate.  
- Build tools to evaluate the effectiveness of LLM-generated jailbreaks.  
- Design evaluation questions to test reasoning abilities in safety-relevant contexts.  
- Contribute to research papers, blog posts, and talks.  
- Support the design and implementation of Anthropic’s Responsible Scaling Policy.  

---

## Required Qualifications  
- Significant software, ML, or research engineering experience.  
- Experience contributing to empirical AI research projects.  
- Familiarity with technical AI safety research.  
- Preference for fast-moving, collaborative projects.  
- A proactive mindset, willing to take on responsibilities beyond your job description.  
- Strong interest in the societal impacts of AI.  

---

## Preferred Qualifications  
- Experience authoring research papers in ML, NLP, or AI safety.  
- Familiarity with large language models (LLMs).  
- Experience with reinforcement learning.  
- Expertise with Kubernetes clusters and complex shared codebases.  

---

## Compensation  
- Annual salary range: £225,000 - £500,000 GBP.  

---

## Logistics  

### Education Requirements  
- A Bachelor’s degree in a related field or equivalent experience is required.  

### Hybrid Work Policy  
- Staff are expected to work in-office at least 25% of the time, with occasional travel to San Francisco for London-based candidates.  

### Visa Sponsorship  
- Visa sponsorship is available, and we provide legal support to facilitate the process for successful candidates.  

---

## Why Anthropic?  
- Collaborative team environment focused on large-scale, impactful AI research.  
- Emphasis on advancing steerable, trustworthy AI systems over smaller, isolated challenges.  
- Research-oriented culture inspired by empirical sciences like physics and biology.  
- Opportunities to contribute to meaningful AI safety efforts with social and ethical implications.  
- Competitive compensation and benefits, including equity donation matching, flexible hours, and generous leave policies.

If you’re passionate about AI alignment and safety, we encourage you to apply — even if you don’t meet every listed qualification.  

---

## Contact Information  
- **Website**: [anthropic.com](https://www.anthropic.com)  
- **Careers**: [anthropic.com/careers](https://www.anthropic.com/careers)  
